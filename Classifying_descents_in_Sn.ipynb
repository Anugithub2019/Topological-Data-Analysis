{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "283b67c7",
      "metadata": {
        "id": "283b67c7"
      },
      "source": [
        "# ML for classifying descent sets of permutations\n",
        "\n",
        "*Goal*: Make a classifier that can predict the descent sets of permutations in some medium-size symmetric group, say $S_{20}$.\n",
        "\n",
        "*Model inputs*: I think that we should be able to represent a permutation in one-line notation, i.e. $[w(1), \\ldots, w(n)]$.\n",
        "We might also think of this as points in the orbit $S_n \\cdot (1, 2, \\ldots, n) \\subseteq \\mathbb{R}^n$.\n",
        "Since we want all inputs to live inside the box $[-1, 1]^n$ for network initialisation reasons, we will scale each point by $1/n$.\n",
        "We're also working in Python and so permuting the set $\\{0, 1, \\ldots, n-1\\}$ will be easier.\n",
        "\n",
        "*Model outputs*: A vector in $\\mathbb{R}^{n-1}$, where each entry is treated like a binary classification of whether the transposition $s_i = (i, i+1)$ is a right descent or not.\n",
        "The output units are logits, we'll apply the [sigmoid function](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) $x \\mapsto \\frac{1}{(1 + e^{-x})}$ componentwise to go from that to a vector of probabilities.\n",
        "So this network should be viewed as a function $\\mathbb{R}^n \\to \\mathbb{R}^{n-1}$, where projecting to the $i$th output only works like a binary classifier $\\mathbb{R}^n \\to \\mathbb{R}$.\n",
        "\n",
        "Author: Joel Gibson<br>\n",
        "Additions to handle left descent sets and permutation matrices by Geordie Williamson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c500c889",
      "metadata": {
        "id": "c500c889"
      },
      "outputs": [],
      "source": [
        "from typing import List, Sequence, Set\n",
        "\n",
        "import numpy as np # numbers, matrices, etc.\n",
        "\n",
        "import pandas as pd # displaying data in a nice way\n",
        "import torch # machine learning stuff\n",
        "import torch.nn as nn # neural net library\n",
        "\n",
        "from numpy.typing import NDArray"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c699a5c",
      "metadata": {
        "id": "5c699a5c"
      },
      "source": [
        "## Generate a dataset\n",
        "\n",
        "We can use `np.random` functions to create random permutations.\n",
        "We'll also be using `np.random.default_rng(seed)` with a `seed` argument to try to get reproducible results (at least reproducible on one machine, not necessarily another), which can be handy for tracking down bugs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e690196e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e690196e",
        "outputId": "0bc2a0a0-79ba-40c3-fda5-941f7bfcbd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First permutation: [4 0 1 2 3]\n",
            "Second permutation: [6 3 5 4 0 1 2]\n",
            "Third permutation: [4 0 1 2 3]\n",
            "Fourth permutation: [6 3 5 4 0 1 2]\n"
          ]
        }
      ],
      "source": [
        "# Create a random number generator with a specific seed, and then create two permutations.\n",
        "# Note that we can reproduce these two \"random\" permutations if we re-seed the generator.\n",
        "rand_gen = np.random.default_rng(seed=1)\n",
        "print(\"First permutation:\", rand_gen.permutation(5))\n",
        "print(\"Second permutation:\", rand_gen.permutation(7))\n",
        "\n",
        "rand_gen = np.random.default_rng(seed=1)\n",
        "print(\"Third permutation:\", rand_gen.permutation(5))\n",
        "print(\"Fourth permutation:\", rand_gen.permutation(7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ebd83f",
      "metadata": {
        "id": "22ebd83f"
      },
      "source": [
        "A convenience function for calculating right and left descent sets of permutations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9275da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9275da",
        "outputId": "46e017f0-8c79-4540-e7c3-828bc9e26fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8 4 7 0 1 2 5 9 6 3] {0, 8, 2, 7}\n",
            "[8 4 7 0 1 2 5 9 6 3] {3, 6, 7}\n"
          ]
        }
      ],
      "source": [
        "def right_descent_set(perm: Sequence[int]) -> Set[int]:\n",
        "    \"\"\"\n",
        "    Return the right descent set, i.e. those i in the range [0, n-1) such that the transposition\n",
        "    (i, i+1) multiplied on the right lowers the length of the permutation.\n",
        "\n",
        "    Multiplication on the right by a transposition (i, j) corresponds to swapping perm[i] and perm[j],\n",
        "    so we can simply check whether we are making a previously out-of-order pair into an in-order pair.\n",
        "\n",
        "    >>> right_descent_set((0, 1, 2))\n",
        "    set()\n",
        "    >>> right_descent_set((2, 1, 0))\n",
        "    {0, 1}\n",
        "    \"\"\"\n",
        "    return {i for i in range(len(perm) - 1) if perm[i] > perm[i+1]}\n",
        "\n",
        "def left_descent_set(perm: Sequence[int]) -> Set[int]:\n",
        "    \"\"\"\n",
        "    Return the left descent set, i.e. those i in the range [0, n-1) such that the transposition\n",
        "    (i, i+1) multiplied on the left lowers the length of the permutation.\n",
        "\n",
        "    Multiplication on the left by a transposition (i, j) corresponds to swapping i and j in the\n",
        "    sequence (perm[0],perm[1],...,perm[n-1]).\n",
        "\n",
        "    so we can simply check whether i occurs to the right of i+1 in this sequence\n",
        "\n",
        "    >>> left_descent_set((0, 1, 2))\n",
        "    set()\n",
        "    >>> right_descent_set((2, 1, 0))\n",
        "    {0, 1}\n",
        "    \"\"\"\n",
        "    perm = tuple(perm)\n",
        "    return {i for i in range(len(perm) - 1) if perm.index(i) > perm.index(i+1)}\n",
        "\n",
        "rand_gen = np.random.default_rng(seed=1)\n",
        "perm = rand_gen.permutation(10)\n",
        "print(perm, right_descent_set(perm))\n",
        "print(perm, left_descent_set(perm))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun we check the symmetry property:"
      ],
      "metadata": {
        "id": "q0dpzlKmfaZx"
      },
      "id": "q0dpzlKmfaZx"
    },
    {
      "cell_type": "code",
      "source": [
        "rand_gen = np.random.default_rng(seed=1)\n",
        "perm = rand_gen.permutation(10)\n",
        "inverse_perm = np.argsort(perm)\n",
        "print(right_descent_set(perm) == left_descent_set(inverse_perm))\n",
        "print(left_descent_set(perm) == right_descent_set(inverse_perm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZ6XSvpfefL",
        "outputId": "2825fb6b-4e3b-442b-d126-bc6aeb034693"
      },
      "id": "ZgZ6XSvpfefL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1001960e",
      "metadata": {
        "id": "1001960e"
      },
      "source": [
        "Let's say that $n$ is the size of the symmetric group, i.e. $S_n$, and $L$ is the number of random permutations we want to generate. Our input tensor should be of shape $(L, n)$ and the output tensor should be of shape $(L, n-1)$. We can wrap these up with a [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html?highlight=tensordataset#torch.utils.data.TensorDataset), which will have utilities for shuffling these two tensors in parallel during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a77f3ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a77f3ba",
        "outputId": "e85e048d-8686-4fe3-854a-8878480a34c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4500, 0.6000, 0.2500, 0.0000, 0.7000, 0.3000, 0.1000, 0.4000, 0.1500,\n",
              "         0.8000, 0.5000, 0.9500, 0.3500, 0.0500, 0.9000, 0.6500, 0.5500, 0.7500,\n",
              "         0.8500, 0.2000]),\n",
              " tensor([0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
              "         1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def generate_dataset(gen: np.random.Generator, n: int, L: int, sidedness: str):\n",
        "    # Generate permutations of {0, ..., n-1}: tensor shape (L, n).\n",
        "    permutations = np.stack([gen.permutation(n) for _ in range(L)])\n",
        "\n",
        "    # Make a (L, n-1) shape tensor of zeros, which we will fill up with the descent sets\n",
        "    descent_sets = np.zeros((L, n-1))\n",
        "    for i in range(L):\n",
        "        if sidedness == 'left':\n",
        "          for s in left_descent_set(permutations[i]):\n",
        "              descent_sets[i, s] = 1\n",
        "        if sidedness == 'right':\n",
        "          for s in right_descent_set(permutations[i]):\n",
        "              descent_sets[i, s] = 1\n",
        "\n",
        "    # Scale the permutations so that they are between 0 and 1, and convert to Pytorch tensors.\n",
        "    return torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(permutations).float() / n,\n",
        "        torch.from_numpy(descent_sets).float()\n",
        "    )\n",
        "\n",
        "# Once created, items can be pulled out of the dataset by indexing [...].\n",
        "# Let's check that one of the rows in the dataset looks correct.\n",
        "dataset = generate_dataset(rand_gen, 20, 1000,'left')\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd8d0f9",
      "metadata": {
        "id": "2cd8d0f9"
      },
      "source": [
        "## Network architecture\n",
        "\n",
        "We'll do a fully-connected network with a configurable number of internal layers, followed by a component-wise sigmoid at the end.\n",
        "I.e. the output of this model is already in probability space rather than logits space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fd474b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fd474b",
        "outputId": "e63ed11e-2b32-4b32-b06d-9d4b518536c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "class FCModel(nn.Module):\n",
        "    def __init__(self, n: int, layer_dims: List[int]):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.layer_dims = layer_dims\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Linear(in_dim, out_dim)\n",
        "            for in_dim, out_dim in zip([n, *layer_dims], [*layer_dims, n - 1])\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "\n",
        "            # We don't want to ReLU the last layer\n",
        "            if i != len(self.layers) - 1:\n",
        "                x = nn.functional.relu(x)\n",
        "\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Test our model has all its sizes etc set up right.\n",
        "model = FCModel(n=20, layer_dims=[100, 10, 5])\n",
        "model(torch.zeros((100, 20))).shape                   # 'batch variable!'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d19563",
      "metadata": {
        "id": "b4d19563"
      },
      "source": [
        "Let's also make a function to count how many predictions the network is making correctly, with some cutoff probability for whether we determine something to be in or out of the set.\n",
        "We count a prediction correct only if it is absolutely the same as the other set.\n",
        "Something which is off by only one element is counted as incorrect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a1d4d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73a1d4d7",
        "outputId": "253ae5d5-735e-4503-cfa9-50da80736074"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def count_correct(model, dataset, cutoff=0.5):\n",
        "    perms, expected = dataset[:]\n",
        "    with torch.no_grad():\n",
        "        classification = model(perms) >= cutoff\n",
        "\n",
        "    total = perms.shape[0]\n",
        "    correct = 0\n",
        "    for i in range(total):\n",
        "        if torch.all(classification[i] == expected[i]):\n",
        "            correct += 1\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "# We would expect a new randomly initialised network to get nothing correct.\n",
        "count_correct(model, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb08132",
      "metadata": {
        "id": "beb08132"
      },
      "source": [
        "## Training hyperparameters\n",
        "\n",
        "The only thing of real note here is the fact that I'm using the binary cross-entropy loss [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), since I want to treat each output as its own binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097583e5",
      "metadata": {
        "id": "097583e5"
      },
      "outputs": [],
      "source": [
        "# Size of symmetric group.\n",
        "n = 35\n",
        "\n",
        "# Train and test data: by setting the random seed here, we get the same data each time.\n",
        "rand_gen = np.random.default_rng(seed=1)\n",
        "train_data = generate_dataset(rand_gen, n, 20_000,'right')\n",
        "test_data = generate_dataset(rand_gen, n, 5_000,'right')\n",
        "\n",
        "# Model shape. Note that PyTorch will randomise the initial model parameters each time here.\n",
        "model = FCModel(n, [500, 100])       # can experiment here!!\n",
        "\n",
        "# Optimisation method.\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5) # can mess with learning rate if you want\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.BCELoss() # binary cross entropy\n",
        "\n",
        "# Epochs to use for training. The whole training dataset will be seen once during each epoch.\n",
        "epochs = 100\n",
        "\n",
        "# Minibatch size: the error over a minibatch will be accumulated before a gradient step.\n",
        "# There are multiple minibatches within an epoch.\n",
        "minibatch_size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919a4027",
      "metadata": {
        "id": "919a4027"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we see that the network learns the right descent set easily (in 100 epochs or so).\n",
        "\n",
        "(Remember that one should rerun the previous cell each time one tries a new experiment, otherwise the network weights are preserved.)"
      ],
      "metadata": {
        "id": "MiUkijBVhihr"
      },
      "id": "MiUkijBVhihr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d5e81c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73d5e81c",
        "outputId": "50866d75-3791-4a9c-f695-b6be51246649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    9: Train loss 0.12, Test loss 0.12,  2086 out of  5000 correct (41.72%)\n",
            "Epoch   19: Train loss 0.05, Test loss 0.05,  4012 out of  5000 correct (80.24%)\n",
            "Epoch   29: Train loss 0.03, Test loss 0.04,  4483 out of  5000 correct (89.66%)\n",
            "Epoch   39: Train loss 0.02, Test loss 0.03,  4871 out of  5000 correct (97.42%)\n",
            "Epoch   49: Train loss 0.02, Test loss 0.02,  4968 out of  5000 correct (99.36%)\n",
            "Epoch   59: Train loss 0.02, Test loss 0.02,  4949 out of  5000 correct (98.98%)\n",
            "Epoch   69: Train loss 0.01, Test loss 0.01,  4836 out of  5000 correct (96.72%)\n",
            "Epoch   79: Train loss 0.01, Test loss 0.01,  4985 out of  5000 correct (99.70%)\n",
            "Epoch   89: Train loss 0.01, Test loss 0.01,  4987 out of  5000 correct (99.74%)\n",
            "Epoch   99: Train loss 0.01, Test loss 0.01,  4990 out of  5000 correct (99.80%)\n"
          ]
        }
      ],
      "source": [
        "train_loss = np.zeros(epochs)\n",
        "test_loss = np.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for perms, expected in torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True):\n",
        "        optimiser.zero_grad()\n",
        "        output = model(perms)\n",
        "        loss = loss_fn(output, expected)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        perms, expected = train_data[:]\n",
        "        train_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "        perms, expected = test_data[:]\n",
        "        test_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        correct, total = count_correct(model, test_data, cutoff=0.5)\n",
        "        print(f\"Epoch {epoch:4}: Train loss {train_loss[epoch]:.2f}, Test loss {test_loss[epoch]:.2f}, {correct:5} out of {total:5} correct ({correct/total:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we switch to left descent sets, and see that the network struggles much more to train.\n",
        "\n",
        "One does get some evidence that it can learn the left descent set if one lowers n and lets it train much more. However the difference between the two tasks is really striking."
      ],
      "metadata": {
        "id": "d4yxDqbFhvJE"
      },
      "id": "d4yxDqbFhvJE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df56ae8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9df56ae8",
        "outputId": "c77ff695-7e6d-4d4b-8314-a8bff1107555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    9: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   19: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   29: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   39: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   49: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   59: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   69: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   79: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   89: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch   99: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n"
          ]
        }
      ],
      "source": [
        "train_data = generate_dataset(rand_gen, n, 20_000,'left')\n",
        "test_data = generate_dataset(rand_gen, n, 5_000,'left')\n",
        "\n",
        "# Model shape. Note that PyTorch will randomise the initial model parameters each time here.\n",
        "model = FCModel(n, [500, 100])       # can experiment here!!\n",
        "\n",
        "train_loss = np.zeros(epochs)\n",
        "test_loss = np.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for perms, expected in torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True):\n",
        "        optimiser.zero_grad()\n",
        "        output = model(perms)\n",
        "        loss = loss_fn(output, expected)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        perms, expected = train_data[:]\n",
        "        train_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "        perms, expected = test_data[:]\n",
        "        test_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        correct, total = count_correct(model, test_data, cutoff=0.5)\n",
        "        print(f\"Epoch {epoch:4}: Train loss {train_loss[epoch]:.2f}, Test loss {test_loss[epoch]:.2f}, {correct:5} out of {total:5} correct ({correct/total:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifying the model to accept permutation matrices.\n",
        "\n",
        "Now we change the model slightly to accept permutation matrices.\n",
        "\n",
        "This allows us to test our hypothesis that the stark difference between left and right descent sets is due to the representation."
      ],
      "metadata": {
        "id": "boKEw8HdihRC"
      },
      "id": "boKEw8HdihRC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780bed78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "780bed78",
        "outputId": "17b55bf8-7ab2-4a06-eb13-bbc3dd575b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. 0. 0. 1. 0. 1. 0.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0.]),\n",
              " tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
              "         0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "def flat_permutation_matrix(perm:tuple) -> NDArray:\n",
        "  n = len(perm)\n",
        "  mat = np.zeros((n,n))\n",
        "  for i, v in enumerate(perm): mat[i,v] = 1\n",
        "  return mat.reshape((n**2))\n",
        "\n",
        "def generate_dataset_matrices(gen: np.random.Generator, n: int, L: int,sidedness:str):\n",
        "    # Generate permutations of {0, ..., n-1}: tensor shape (L, n).\n",
        "    permutations = np.stack([gen.permutation(n) for _ in range(L)])\n",
        "\n",
        "    permutation_mats = np.stack([flat_permutation_matrix(p) for p in permutations])\n",
        "\n",
        "    # Make a (L, n-1) shape tensor of zeros, which we will fill up with the descent sets\n",
        "    descent_sets = np.zeros((L, n-1))\n",
        "    for i in range(L):\n",
        "        if sidedness == 'left':\n",
        "          for s in left_descent_set(permutations[i]):\n",
        "              descent_sets[i, s] = 1\n",
        "        if sidedness == 'right':\n",
        "          for s in right_descent_set(permutations[i]):\n",
        "              descent_sets[i, s] = 1\n",
        "\n",
        "    # Scale the permutations so that they are between 0 and 1, and convert to Pytorch tensors.\n",
        "    return torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(permutation_mats).float(),\n",
        "        torch.from_numpy(descent_sets).float()\n",
        "    )\n",
        "\n",
        "# Once created, items can be pulled out of the dataset by indexing [...].\n",
        "# Let's check that one of the rows in the dataset looks correct.\n",
        "dataset = generate_dataset_matrices(rand_gen, 20, 1000,'left')\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FCModel_matrices(nn.Module):\n",
        "    def __init__(self, n: int, layer_dims: List[int]):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "        self.layer_dims = layer_dims\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Linear(in_dim, out_dim)\n",
        "            for in_dim, out_dim in zip([n**2, *layer_dims], [*layer_dims, n - 1])\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "\n",
        "            # We don't want to ReLU the last layer\n",
        "            if i != len(self.layers) - 1:\n",
        "                x = nn.functional.relu(x)\n",
        "\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Test our model has all its sizes etc set up right.\n",
        "model = FCModel_matrices(n=20, layer_dims=[100, 10, 5])\n",
        "model(torch.zeros((100, 20**2))).shape                   # 'batch variable!'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6AZoN5vmapw",
        "outputId": "b9449206-c584-43e1-e312-fe6c1ebf7a43"
      },
      "id": "P6AZoN5vmapw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_correct(model, dataset, cutoff=0.5):\n",
        "    perms, expected = dataset[:]\n",
        "    with torch.no_grad():\n",
        "        classification = model(perms) >= cutoff\n",
        "\n",
        "    total = perms.shape[0]\n",
        "    correct = 0\n",
        "    for i in range(total):\n",
        "        if torch.all(classification[i] == expected[i]):\n",
        "            correct += 1\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "# We would expect a new randomly initialised network to get nothing correct.\n",
        "count_correct(model, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYjgC1LenFdC",
        "outputId": "2d883e6a-015f-42fe-cccb-3c38c78b7fa4"
      },
      "id": "pYjgC1LenFdC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of symmetric group.\n",
        "# Note: we make this smaller than above.\n",
        "\n",
        "n = 20\n",
        "\n",
        "# Train and test data: by setting the random seed here, we get the same data each time.\n",
        "rand_gen = np.random.default_rng(seed=1)\n",
        "train_data = generate_dataset_matrices(rand_gen, n, 20_000,'left')\n",
        "test_data = generate_dataset_matrices(rand_gen, n, 5_000,'left')\n",
        "\n",
        "# Model shape. Note that PyTorch will randomise the initial model parameters each time here.\n",
        "model = FCModel_matrices(n, [1000, 100])       # can experiment here!!\n",
        "\n",
        "# Optimisation method.\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5) # can mess with learning rate if you want\n",
        "\n",
        "# Loss function.\n",
        "loss_fn = nn.BCELoss() # binary cross entropy\n",
        "\n",
        "# Epochs to use for training. The whole training dataset will be seen once during each epoch.\n",
        "epochs = 100\n",
        "\n",
        "# Minibatch size: the error over a minibatch will be accumulated before a gradient step.\n",
        "# There are multiple minibatches within an epoch.\n",
        "minibatch_size = 50"
      ],
      "metadata": {
        "id": "1T4mugxKnsPi"
      },
      "id": "1T4mugxKnsPi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, in comparison to our earlier experiments, the input dimension is now $n^2$-dimensional, rather than $n$-dimensional. Hence we probably need a larger first hidden layer to accommodate this. We did this by replacing 500 by 1000 above. This leads to slightly longer training times.\n",
        "\n",
        "If one runs the previous cell and the next, one sees (almost) no difference between 'left' and 'right' descents. Also accuracy is very good after 100 epochs of training. This appears to confirm the hypothesis that the representation really was the issue in our previous experiments."
      ],
      "metadata": {
        "id": "E6LmDUbWpb9T"
      },
      "id": "E6LmDUbWpb9T"
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = np.zeros(epochs)\n",
        "test_loss = np.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for perms, expected in torch.utils.data.DataLoader(train_data, batch_size=minibatch_size, shuffle=True):\n",
        "        optimiser.zero_grad()\n",
        "        output = model(perms)\n",
        "        loss = loss_fn(output, expected)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        perms, expected = train_data[:]\n",
        "        train_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "        perms, expected = test_data[:]\n",
        "        test_loss[epoch] = float(loss_fn(model(perms), expected))\n",
        "\n",
        "    if epoch % 10 == 9 or epoch < 10:\n",
        "        correct, total = count_correct(model, test_data, cutoff=0.5)\n",
        "        print(f\"Epoch {epoch:4}: Train loss {train_loss[epoch]:.2f}, Test loss {test_loss[epoch]:.2f}, {correct:5} out of {total:5} correct ({correct/total:.2%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6JHc41coAmh",
        "outputId": "b22387c7-c148-4759-cc28-77806e4ca069"
      },
      "id": "Z6JHc41coAmh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0: Train loss 0.69, Test loss 0.69,     0 out of  5000 correct (0.00%)\n",
            "Epoch    1: Train loss 0.69, Test loss 0.69,     2 out of  5000 correct (0.04%)\n",
            "Epoch    2: Train loss 0.68, Test loss 0.68,     7 out of  5000 correct (0.14%)\n",
            "Epoch    3: Train loss 0.65, Test loss 0.65,    49 out of  5000 correct (0.98%)\n",
            "Epoch    4: Train loss 0.55, Test loss 0.55,    88 out of  5000 correct (1.76%)\n",
            "Epoch    5: Train loss 0.39, Test loss 0.40,   210 out of  5000 correct (4.20%)\n",
            "Epoch    6: Train loss 0.28, Test loss 0.29,   602 out of  5000 correct (12.04%)\n",
            "Epoch    7: Train loss 0.21, Test loss 0.21,  1210 out of  5000 correct (24.20%)\n",
            "Epoch    8: Train loss 0.16, Test loss 0.17,  1813 out of  5000 correct (36.26%)\n",
            "Epoch    9: Train loss 0.13, Test loss 0.14,  2259 out of  5000 correct (45.18%)\n",
            "Epoch   19: Train loss 0.04, Test loss 0.04,  4322 out of  5000 correct (86.44%)\n",
            "Epoch   29: Train loss 0.02, Test loss 0.02,  4785 out of  5000 correct (95.70%)\n",
            "Epoch   39: Train loss 0.01, Test loss 0.01,  4930 out of  5000 correct (98.60%)\n",
            "Epoch   49: Train loss 0.01, Test loss 0.01,  4955 out of  5000 correct (99.10%)\n",
            "Epoch   59: Train loss 0.00, Test loss 0.01,  4961 out of  5000 correct (99.22%)\n",
            "Epoch   69: Train loss 0.00, Test loss 0.01,  4966 out of  5000 correct (99.32%)\n",
            "Epoch   79: Train loss 0.00, Test loss 0.01,  4970 out of  5000 correct (99.40%)\n",
            "Epoch   89: Train loss 0.00, Test loss 0.01,  4974 out of  5000 correct (99.48%)\n",
            "Epoch   99: Train loss 0.00, Test loss 0.01,  4974 out of  5000 correct (99.48%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}